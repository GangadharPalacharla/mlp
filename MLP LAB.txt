1. FIND - S ALGORITHM :



students.csv



data = """ex,PG,year,CGPA,sports,interest,scholarship
1,MCA,2,high,yes,Dance,yes
2,MCA,2,high,no,Dance,yes
3,MCA,1,high,yes,Music,no
4,MBA,2,high,yes,Dance,no
5,MCA,2,medium,yes,Dance,yes
"""

with open("students.csv", "w") as f:
    f.write(data)

print("students.csv created successfully!")



CODE :



import pandas as pd
df = pd.read_csv("students.csv")
pos = df[df['scholarship'].str.lower() == 'yes']
hypothesis = list(pos.iloc[0, :-1])
for i, row in pos.iterrows():
    example = list(row[:-1])
    for j in range(len(hypothesis)):
        if hypothesis[j] != example[j]:
            hypothesis[j] = '?'
print("Final hypothesis:", hypothesis)






2. Candidate-Elimination algorithm to output a description of the set of all hypotheses consistent with the training examples




students.csv



data = """ex,PG,year,CGPA,sports,interest,scholarship
1,MCA,2,high,yes,Dance,yes
2,MCA,2,high,no,Dance,yes
3,MCA,1,high,yes,Music,no
4,MBA,2,high,yes,Dance,no
5,MCA,2,medium,yes,Dance,yes
"""

with open("students.csv", "w") as f:
    f.write(data)

print("students.csv created successfully!")




CODE:



import csv

data = []
with open("weather.csv", "r") as f:
    for row in csv.reader(f):
        data.append(row)

data = data[1:]

S = ['0'] * (len(data[0]) - 1)
G = ['?'] * (len(data[0]) - 1)

for row in data:
    attrs = row[:-1]
    target = row[-1]

    if target == "yes":
        for i in range(len(S)):
            if S[i] == '0':
                S[i] = attrs[i]
            elif S[i] != attrs[i]:
                S[i] = '?'
    else:
        for i in range(len(S)):
            if G[i] != '?' and S[i] != attrs[i]:
                G[i] = '?'

print("Final Specific Hypothesis:", S)
print("Final General Hypothesis:", G)





 
EXPERIMENT 3 – ID3 Decision Tree



data_id3.csv



data = """CGPA,Sports,Projects,Attendance,Scholarship
1,1,1,1,yes
1,0,1,1,yes
1,0,0,1,no
0,1,1,1,yes
0,1,0,0,no
"""

with open("data_id3.csv", "w") as f:
    f.write(data)

print("data_id3.csv created successfully!")




CODE :



import pandas as pd
from sklearn.tree import DecisionTreeClassifier, export_text
df = pd.read_csv("data_id3.csv")
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
model = DecisionTreeClassifier(criterion="entropy")
model.fit(X, y)
print(export_text(model, feature_names=list(X.columns)))
new_sample = [[1,2,1,0]]
print("Prediction:", model.predict(new_sample))







EXPERIMENT 4 – KNN (Iris)



CODE :



from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
print("Correct:", (y_pred == y_test).sum())
print("Wrong:", (y_pred != y_test).sum())
print("Accuracy:", accuracy_score(y_test, y_pred))






EXPERIMENT 5 – ANN Backpropagation (MLP)



CODE:



from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
mlp = MLPClassifier(hidden_layer_sizes=(5,), max_iter=1000, random_state=0)
mlp.fit(X_train, y_train)
y_pred = mlp.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))



EXPERIMENT 5(sunita mam vaste idi execute cheyandi!!!)
import numpy as np

X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])

y = np.array([[0],
              [1],
              [1],
              [1]])

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

w1 = np.random.rand()
w2 = np.random.rand()
w3 = np.random.rand()

lr = 0.1

for epoch in range(5000):
    for i in range(4):
        hidden = sigmoid(X[i][0] * w1 + X[i][1] * w2)
        output = sigmoid(hidden * w3)
        error = y[i] - output
        d_output = error * sigmoid_derivative(output)
        d_hidden = d_output * w3 * sigmoid_derivative(hidden)
        w3 += lr * hidden * d_output
        w1 += lr * X[i][0] * d_hidden
        w2 += lr * X[i][1] * d_hidden

print("Final Predictions:")
for i in range(4):
    hidden = sigmoid(X[i][0] * w1 + X[i][1] * w2)
    output = sigmoid(hidden * w3)
    print(X[i], "→", round(float(output), 3))



EXPERIMENT 6 – Naive Bayes(CSV)



nb_data.csv



data = """Feature1,Feature2,Class
1,1,yes
1,0,no
0,1,yes
0,0,no
"""

with open("nb_data.csv", "w") as f:
    f.write(data)

print("nb_data.csv created successfully!")




CODE:



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
df = pd.read_csv("nb_data.csv")
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
nb = GaussianNB()
nb.fit(X_train, y_train)
y_pred = nb.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))





EXPERIMENT 7 – Bayesian Network (Heart)



heart.csv



data = """Age,Cholesterol,BP,HeartDisease
1,1,1,yes
1,1,0,yes
1,0,1,yes
1,0,0,no
0,1,1,yes
0,1,0,no
"""

with open("heart.csv", "w") as f:
    f.write(data)

print("heart.csv created successfully!")




CODE :



import pandas as pd
from pgmpy.models import BayesianNetwork
from pgmpy.estimators import MaximumLikelihoodEstimator
from pgmpy.inference import VariableElimination
data = pd.read_csv("heart.csv")
model = BayesianNetwork([
    ('Age', 'HeartDisease'),
    ('Cholesterol', 'HeartDisease'),
    ('BP', 'HeartDisease')
])
model.fit(data, estimator=MaximumLikelihoodEstimator)
infer = VariableElimination(model)
q = infer.query(variables=['HeartDisease'],
                evidence={'Age':1,'Cholesterol':1,'BP':0})
print(q)






EXPERIMENT 8 – Naive Bayes Text Classification



CODE :



from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
docs=[
"this book is good",
"this movie is bad",
"i love this film",
"i hate this book",
"good story and good acting",
"worst film ever"
]
labels=['pos','neg','pos','neg','pos','neg']
vec=TfidfVectorizer()
X=vec.fit_transform(docs)
X_train,X_test,y_train,y_test=train_test_split(X,labels,test_size=0.3,random_state=0)
clf=MultinomialNB()
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)
print("Accuracy:",accuracy_score(y_test,y_pred))
test_doc=vec.transform(["the story is good"])
print("Class:",clf.predict(test_doc)[0])






EXPERIMENT 9 – EM (GMM) vs KMeans




CODE :



from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
X,_=make_blobs(n_samples=200,centers=3,random_state=0)
kmeans=KMeans(n_clusters=3,random_state=0)
kmeans_labels=kmeans.fit_predict(X)
gmm=GaussianMixture(n_components=3,random_state=0)
gmm_labels=gmm.fit_predict(X)
print("Silhouette KMeans:",silhouette_score(X,kmeans_labels))
print("Silhouette GMM:",silhouette_score(X,gmm_labels))





EXPERIMENT 10 – LWR



CODE :





import numpy as np
import matplotlib.pyplot as plt

def lwlr(x_train, y_train, x_query, tau):
    y_pred = []
    for x0 in x_query:
        w = np.exp(- (x_train - x0)**2 / (2 * tau**2))
        W = np.diag(w)
        X = np.c_[np.ones(len(x_train)), x_train]
        theta = np.linalg.pinv(X.T @ W @ X) @ (X.T @ W @ y_train)
        y_pred.append([1, x0] @ theta)
    return np.array(y_pred)

x_train = np.array([1,2,3,4,5], float)
y_train = np.array([1,4,9,16,25], float)

x_query = np.linspace(1,5,100)
y_query = lwlr(x_train, y_train, x_query, tau=0.5)

plt.scatter(x_train, y_train)
plt.plot(x_query, y_query)
plt.show()





